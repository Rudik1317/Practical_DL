{"cells":[{"cell_type":"markdown","metadata":{"id":"TxtSz3W5BpXQ"},"source":["## Seminar part 1: Fun with Word Embeddings\n","\n","Today we gonna play with word embeddings: train our own little embedding, load one from   gensim model zoo and use it to visualize text corpora.\n","\n","This whole thing is gonna happen on top of embedding dataset.\n","\n","__Requirements:__  `pip install --upgrade nltk gensim bokeh` , but only if you're running locally."]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"LwwDrVxfBpXZ","executionInfo":{"status":"ok","timestamp":1698242341456,"user_tz":-180,"elapsed":924,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"78e928bd-d4b7-4378-90be-87ff0c04ade3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-25 13:58:59--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/dl/obaitrix9jyu84r/quora.txt [following]\n","--2023-10-25 13:58:59--  https://www.dropbox.com/s/dl/obaitrix9jyu84r/quora.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucfdbb4a672bcebc9bb1caf2af40.dl.dropboxusercontent.com/cd/0/get/CGRaBMns8barNW_QlJqZ2ONsbVpZlIxjszISf9YMtYZ8OxC7JlDXLrP2vYI_X3dYkSRF7XMnenBzjhRY0I6dw81ovRI7yE4TOsxiaWa6S4ACBqW0giH3SSG9vhc3UbnyOs8/file?dl=1# [following]\n","--2023-10-25 13:59:00--  https://ucfdbb4a672bcebc9bb1caf2af40.dl.dropboxusercontent.com/cd/0/get/CGRaBMns8barNW_QlJqZ2ONsbVpZlIxjszISf9YMtYZ8OxC7JlDXLrP2vYI_X3dYkSRF7XMnenBzjhRY0I6dw81ovRI7yE4TOsxiaWa6S4ACBqW0giH3SSG9vhc3UbnyOs8/file?dl=1\n","Resolving ucfdbb4a672bcebc9bb1caf2af40.dl.dropboxusercontent.com (ucfdbb4a672bcebc9bb1caf2af40.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n","Connecting to ucfdbb4a672bcebc9bb1caf2af40.dl.dropboxusercontent.com (ucfdbb4a672bcebc9bb1caf2af40.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 33813903 (32M) [application/binary]\n","Saving to: ‘./quora.txt’\n","\n","./quora.txt         100%[===================>]  32.25M  85.0MB/s    in 0.4s    \n","\n","2023-10-25 13:59:01 (85.0 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n","\n"]}],"source":["# download the data:\n","!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n","# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"]},{"cell_type":"code","execution_count":2,"metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"toO2oSf_BpXc","executionInfo":{"status":"ok","timestamp":1698242342443,"user_tz":-180,"elapsed":1003,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"c2881147-f335-40f0-a1e6-1c50c2b15dbe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"What TV shows or books help you read people's body language?\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["import numpy as np\n","\n","data = list(open(\"./quora.txt\"))\n","data[50]"]},{"cell_type":"markdown","metadata":{"id":"-dTW0_s7BpXe"},"source":["__Tokenization:__ a typical first step for an nlp task is to split raw data into words.\n","The text we're working with is in raw format: with all the punctuation and smiles attached to some words, so a simple str.split won't do.\n","\n","Let's use __`nltk`__ - a library that handles many nlp tasks like tokenization, stemming or part-of-speech tagging."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cswe8Ht_BpXf","executionInfo":{"status":"ok","timestamp":1698242344518,"user_tz":-180,"elapsed":2080,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"137182ab-d680-4fcc-c990-233668a5d1a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"]}],"source":["from nltk.tokenize import WordPunctTokenizer\n","tokenizer = WordPunctTokenizer()\n","\n","print(tokenizer.tokenize(data[50]))"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":true,"id":"PMIG88rdBpXh","executionInfo":{"status":"ok","timestamp":1698242779928,"user_tz":-180,"elapsed":6169,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}}},"outputs":[],"source":["# TASK: lowercase everything and extract tokens with tokenizer.\n","# data_tok should be a list of lists of tokens for each line in data.\n","\n","data_tok = [tokenizer.tokenize(d.lower()) for d in data]"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"id":"ILBH3PouBpXi","executionInfo":{"status":"ok","timestamp":1698242781987,"user_tz":-180,"elapsed":2062,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}}},"outputs":[],"source":["assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n","assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n","is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n","assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qO_hjq8gBpXi","executionInfo":{"status":"ok","timestamp":1698242786690,"user_tz":-180,"elapsed":282,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"e5f2929e-0292-451d-c298-93ae328cccce"},"outputs":[{"output_type":"stream","name":"stdout","text":["[\"can i get back with my ex even though she is pregnant with another guy ' s baby ?\", 'what are some ways to overcome a fast food addiction ?']\n"]}],"source":["print([' '.join(row) for row in data_tok[:2]])"]},{"cell_type":"markdown","metadata":{"id":"TQ6NG_tuBpXj"},"source":["__Word vectors:__ as the saying goes, there's more than one way to train word embeddings. There's Word2Vec and GloVe with different objective functions. Then there's fasttext that uses character-level models to train word embeddings.\n","\n","The choice is huge, so let's start someplace small: __gensim__ is another nlp library that features many vector-based models incuding word2vec."]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"id":"RvFsqRtIBpXk","executionInfo":{"status":"ok","timestamp":1698242830479,"user_tz":-180,"elapsed":41529,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}}},"outputs":[],"source":["from gensim.models import Word2Vec\n","model = Word2Vec(data_tok,\n","                 vector_size=32,      # embedding vector size\n","                 min_count=5,  # consider words that occured at least 5 times\n","                 window=5).wv  # define context as a 5-word window around the target word"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIBhGFFlBpXl","executionInfo":{"status":"ok","timestamp":1698243030128,"user_tz":-180,"elapsed":269,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"d0355d10-9962-4417-a5b2-c444565ef6f4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-2.522757  , -0.5296677 ,  1.8315358 ,  2.197109  ,  1.6816372 ,\n","        1.3080425 ,  1.82537   , -1.3385319 ,  2.1725667 ,  1.7584703 ,\n","       -3.0148811 ,  2.9479187 ,  4.61248   ,  1.2176129 ,  2.00873   ,\n","       -1.7256964 , -0.64040536, -2.6561847 ,  0.57672477, -2.30099   ,\n","       -2.4941416 , -0.08821454, -0.02218496, -1.1490109 ,  1.2683514 ,\n","       -3.480393  , -0.19467475, -0.7655715 ,  0.5910107 ,  2.4765377 ,\n","       -1.6053541 ,  0.04272359], dtype=float32)"]},"metadata":{},"execution_count":12}],"source":["# now you can get word vectors !\n","model.get_vector('anything')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YpNePPzBpXl","executionInfo":{"status":"ok","timestamp":1698243032473,"user_tz":-180,"elapsed":283,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"3079f531-78d3-428f-adbd-e783d67ea46c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('rice', 0.9543619155883789),\n"," ('sauce', 0.9508453011512756),\n"," ('butter', 0.9361094832420349),\n"," ('cheese', 0.9228488206863403),\n"," ('corn', 0.9197880029678345),\n"," ('chicken', 0.9187420606613159),\n"," ('fruit', 0.9177649021148682),\n"," ('honey', 0.9159911870956421),\n"," ('vodka', 0.9123071432113647),\n"," ('garlic', 0.9060683846473694)]"]},"metadata":{},"execution_count":13}],"source":["# or query similar words directly. Go play with it!\n","model.most_similar('bread')"]},{"cell_type":"markdown","metadata":{"id":"rB73pRwQBpXl"},"source":["### Using pre-trained model\n","\n","Took it a while, huh? Now imagine training life-sized (100~300D) word embeddings on gigabytes of text: wikipedia articles or twitter posts.\n","\n","Thankfully, nowadays you can get a pre-trained word embedding model in 2 lines of code (no sms required, promise)."]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"tRIGtGk1BpXm","executionInfo":{"status":"ok","timestamp":1698243271032,"user_tz":-180,"elapsed":234943,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"9ebd676f-cb74-48fb-af74-cc1162b449c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 387.1/387.1MB downloaded\n"]}],"source":["import gensim.downloader as api\n","model = api.load('glove-twitter-100')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhkoUmG0BpXm","executionInfo":{"status":"ok","timestamp":1698243271521,"user_tz":-180,"elapsed":493,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"dc8f0225-4f54-4823-b18f-14a7d64f818e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('broker', 0.5820155739784241),\n"," ('bonuses', 0.5424473285675049),\n"," ('banker', 0.5385112762451172),\n"," ('designer', 0.5197198390960693),\n"," ('merchandising', 0.4964233338832855),\n"," ('treet', 0.4922019839286804),\n"," ('shopper', 0.4920562207698822),\n"," ('part-time', 0.4912828207015991),\n"," ('freelance', 0.4843311905860901),\n"," ('aupair', 0.4796452522277832)]"]},"metadata":{},"execution_count":15}],"source":["model.most_similar(positive=[\"coder\", \"money\"], negative=[\"brain\"])"]},{"cell_type":"markdown","metadata":{"id":"McSC0cUfBpXn"},"source":["### Visualizing word vectors\n","\n","One way to see if our vectors are any good is to plot them. Thing is, those vectors are in 30D+ space and we humans are more used to 2-3D.\n","\n","Luckily, we machine learners know about __dimensionality reduction__ methods.\n","\n","Let's use that to plot 1000 most frequent words"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422},"id":"oaKI75YlBpXn","executionInfo":{"status":"error","timestamp":1698245114293,"user_tz":-180,"elapsed":8,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"521c6098-c4ef-4d90-9d52-6776027d6a8b"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-25f27b64f696>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m words = sorted(model.vocab.keys(), \n\u001b[0m\u001b[1;32m      2\u001b[0m                \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                reverse=True)[:1000]\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"]}],"source":["words = sorted(model.vocab.keys(),\n","               key=lambda word: model.vocab[word].count,\n","               reverse=True)[:1000]\n","\n","print(words[::100])"]},{"cell_type":"code","source":["model.vocab.keys()"],"metadata":{"id":"U13kdV6pTAjx","executionInfo":{"status":"error","timestamp":1698245119594,"user_tz":-180,"elapsed":6,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"c67f663e-9367-4566-e610-82103c0727b9","colab":{"base_uri":"https://localhost:8080/","height":348}},"execution_count":21,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-10583311086b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzQLheoUBpXn"},"outputs":[],"source":["# for each word, compute it's vector with model\n","word_vectors = # YOUR CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"AP1KuA8eBpXo"},"outputs":[],"source":["assert isinstance(word_vectors, np.ndarray)\n","assert word_vectors.shape == (len(words), 100)\n","assert np.isfinite(word_vectors).all()"]},{"cell_type":"markdown","metadata":{"id":"2QryAw69BpXo"},"source":["#### Linear projection: PCA\n","\n","The simplest linear dimensionality reduction method is __P__rincipial __C__omponent __A__nalysis.\n","\n","In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n","\n","<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">\n","\n","\n","Under the hood, it attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n","\n","$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n","- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n","- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n","- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n","- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"xaobU7tVBpXo"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n","# after that, normalize vectors to make sure they have zero mean and unit variance\n","word_vectors_pca = # YOUR CODE\n","\n","# and maybe MORE OF YOUR CODE here :)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4TK9RX0-BpXp"},"outputs":[],"source":["assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n","assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n","assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\""]},{"cell_type":"markdown","metadata":{"id":"sM3MbmvaBpXp"},"source":["#### Let's draw it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8jktMB9BpXp"},"outputs":[],"source":["import bokeh.models as bm, bokeh.plotting as pl\n","from bokeh.io import output_notebook\n","output_notebook()\n","\n","def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n","                 width=600, height=400, show=True, **kwargs):\n","    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n","    if isinstance(color, str): color = [color] * len(x)\n","    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n","\n","    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n","    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n","\n","    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n","    if show: pl.show(fig)\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDJOOAoiBpXp"},"outputs":[],"source":["draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n","\n","# hover a mouse over there and see if you can identify the clusters"]},{"cell_type":"markdown","metadata":{"id":"g9laG5QcBpXq"},"source":["### Visualizing neighbors with t-SNE\n","PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n","\n","If we instead want to focus on keeping neighboring points near, we could use TSNE, which is itself an embedding method. Here you can read __[more on TSNE](https://distill.pub/2016/misread-tsne/)__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7q2S3gl-BpXq"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","# map word vectors onto 2d plane with TSNE. hint: use verbose=100 to see what it's doing.\n","# normalize them as just lke with pca\n","\n","\n","word_tsne = #YOUR CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":false,"id":"PGmooSQjBpXq"},"outputs":[],"source":["draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"]},{"cell_type":"markdown","metadata":{"id":"jbI-C_52BpXq"},"source":["### Visualizing phrases\n","\n","Word embeddings can also be used to represent short phrases. The simplest way is to take __an average__ of vectors for all tokens in the phrase with some weights.\n","\n","This trick is useful to identify what data are you working with: find if there are any outliers, clusters or other artefacts.\n","\n","Let's try this new hammer on our data!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"L-X43L2_BpXr"},"outputs":[],"source":["def get_phrase_embedding(phrase):\n","    \"\"\"\n","    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n","    \"\"\"\n","    # 1. lowercase phrase\n","    # 2. tokenize phrase\n","    # 3. average word vectors for all words in tokenized phrase\n","    # skip words that are not in model's vocabulary\n","    # if all words are missing from vocabulary, return zeros\n","\n","    vector = np.zeros([model.vector_size], dtype='float32')\n","\n","    # YOUR CODE\n","\n","    return vector\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"xB0nexEBBpXr"},"outputs":[],"source":["vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n","\n","assert np.allclose(vector[::10],\n","                   np.array([ 0.31807372, -0.02558171,  0.0933293 , -0.1002182 , -1.0278689 ,\n","                             -0.16621883,  0.05083408,  0.17989802,  1.3701859 ,  0.08655966],\n","                              dtype=np.float32))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"JUUavZwxBpXr"},"outputs":[],"source":["# let's only consider ~5k phrases for a first run.\n","chosen_phrases = data[::len(data) // 1000]\n","\n","# compute vectors for chosen phrases\n","phrase_vectors = # YOUR CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"BvyvfRufBpXs"},"outputs":[],"source":["assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n","assert phrase_vectors.shape == (len(chosen_phrases), model.vector_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ym57s3BNBpXs"},"outputs":[],"source":["# map vectors into 2d space with pca, tsne or your other method of choice\n","# don't forget to normalize\n","\n","phrase_vectors_2d = TSNE(verbose=1000).fit_transform(phrase_vectors)\n","\n","phrase_vectors_2d = (phrase_vectors_2d - phrase_vectors_2d.mean(axis=0)) / phrase_vectors_2d.std(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"34tQTgs8BpXs"},"outputs":[],"source":["draw_vectors(phrase_vectors_2d[:, 0], phrase_vectors_2d[:, 1],\n","             phrase=[phrase[:50] for phrase in chosen_phrases],\n","             radius=20,)"]},{"cell_type":"markdown","metadata":{"id":"cjHip4ZTBpXt"},"source":["Finally, let's build a simple \"similar question\" engine with phrase embeddings we've built."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"znoMZrOqBpXt"},"outputs":[],"source":["# compute vector embedding for all lines in data\n","data_vectors = np.array([get_phrase_embedding(l) for l in data])"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ie4jxYB6BpXt"},"outputs":[],"source":["def find_nearest(query, k=10):\n","    \"\"\"\n","    given text line (query), return k most similar lines from data, sorted from most to least similar\n","    similarity should be measured as cosine between query and line embedding vectors\n","    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n","    \"\"\"\n","    # YOUR CODE\n","\n","    return <YOUR CODE: top-k lines starting from most similar>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ze3FvtgvBpXt"},"outputs":[],"source":["results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n","\n","print(''.join(results))\n","\n","assert len(results) == 10 and isinstance(results[0], str)\n","assert results[0] == 'How do I get to the dark web?\\n'\n","assert results[3] == 'What can I do to save the world?\\n'"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"_DsW0AiUBpX0"},"outputs":[],"source":["find_nearest(query=\"How does Trump?\", k=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"V7Fu_AqHBpX1"},"outputs":[],"source":["find_nearest(query=\"Why don't i ask a question myself?\", k=10)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"9R4V44dyBpX1"},"source":["__Now what?__\n","* Try running TSNE on all data, not just 1000 phrases\n","* See what other embeddings are there in the model zoo: `gensim.downloader.info()`\n","* Take a look at [FastText](https://github.com/facebookresearch/fastText) embeddings\n","* Optimize find_nearest with locality-sensitive hashing: use [nearpy](https://github.com/pixelogik/NearPy) or `sklearn.neighbors`."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}